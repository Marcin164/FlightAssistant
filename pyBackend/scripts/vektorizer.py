"""
Skrypt do wektoryzacji dokument√≥w txt i zapisania wektor√≥w do pliku.
Pozwala na poszukiwanie dokument√≥w za pomocƒÖ cosine similarity.
"""

import numpy as np
import os
import pandas as pd
import pickle
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class DocumentVectorizer:
    """
    Klasa do wektoryzacji dokument√≥w i przechowywania/przeszukiwania wektor√≥w.
    """

    def __init__(self, max_features=8000, ngram_range=(1, 2)):
        """
        Inicjalizacja wektoryzatora.

        Args:
            max_features: Maksymalna liczba cech do wyodrƒôbnienia
            ngram_range: Zakres n-gram√≥w (1-grams i 2-grams)
        """
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=1,
            max_df=0.95,
            stop_words=None,
            lowercase=True
        )
        self.vectors = None
        self.documents = None
        self.metadata = None

    def load_documents(self, folder_path):
        """
        Wczytaj wszystkie pliki txt z folderu.

        Args:
            folder_path: ≈öcie≈ºka do folderu z plikami txt

        Returns:
            Lista dokument√≥w i ich metadanych
        """
        documents = []
        metadata = []

        folder = Path(folder_path)
        txt_files = list(folder.glob("*.txt"))

        if not txt_files:
            print(f"‚ö†Ô∏è  Nie znaleziono plik√≥w txt w {folder_path}")
            return documents, metadata

        print(f"üìÇ Znalezione {len(txt_files)} pliku(√≥w) txt")

        for file_path in sorted(txt_files):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        continue

                    chunks = self.chunk_text(content)

                    for idx, chunk in enumerate(chunks):
                        documents.append(chunk)
                        metadata.append({
                            'filename': file_path.name,
                            'path': str(file_path),
                            'size': os.path.getsize(file_path),
                            'chunk_id': idx,
                            'chunk_count': len(chunks)
                        })

                    print(f"‚úÖ {file_path.name}: {len(chunks)} chunk(s)")

            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd przy wczytywaniu {file_path.name}: {e}")

        self.documents = documents
        self.metadata = metadata
        return documents, metadata

    def vectorize(self):
        """
        Wektoryzuj za≈Çadowane dokumenty.

        Returns:
            Macierz wektor√≥w (sparse matrix)
        """
        if not self.documents:
            raise ValueError("Brak za≈Çadowanych dokument√≥w. U≈ºyj load_documents() najpierw.")

        print(f"\nüîÑ Wektoryzujƒô {len(self.documents)} dokument(√≥w)...")
        self.vectors = self.vectorizer.fit_transform(self.documents)
        print(f"‚úÖ Wektoryzacja uko≈Ñczona. Wymiary: {self.vectors.shape}")

        return self.vectors

    def save(self, output_dir='./vectors_db'):
        """
        Zapisz wektory i metadane do pliku.

        Args:
            output_dir: Katalog do zapisu
        """
        if self.vectors is None:
            raise ValueError("Brak wektor√≥w. Wykonaj vectorize() najpierw.")

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # Zapisz wektory
        vectors_file = output_path / 'vectors.pkl'
        with open(vectors_file, 'wb') as f:
            pickle.dump(self.vectors, f)
        print(f"‚úÖ Wektory zapisane: {vectors_file}")

        # Zapisz wektoryzator
        vectorizer_file = output_path / 'vectorizer.pkl'
        with open(vectorizer_file, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        print(f"‚úÖ Wektoryzator zapisany: {vectorizer_file}")

        # Zapisz metadane jako CSV
        metadata_file = output_path / 'metadata.csv'
        df_metadata = pd.DataFrame(self.metadata)
        df_metadata.to_csv(metadata_file, index=False)
        print(f"‚úÖ Metadane zapisane: {metadata_file}")

        # Zapisz statystykƒô
        stats_file = output_path / 'stats.txt'
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"Liczba dokument√≥w: {len(self.documents)}\n")
            f.write(f"Wymiary wektor√≥w: {self.vectors.shape}\n")
            f.write(f"Liczba cech (features): {self.vectorizer.get_feature_names()}\n")
        print(f"‚úÖ Statystyka zapisana: {stats_file}")

        return output_path

    def load(self, vectors_dir='./vectors_db'):
        """
        Wczytaj wektory z pliku.

        Args:
            vectors_dir: Katalog z zapisanymi wektorami
        """
        vectors_dir = Path(vectors_dir)

        # Wczytaj wektory
        with open(vectors_dir / 'vectors.pkl', 'rb') as f:
            self.vectors = pickle.load(f)

        # Wczytaj wektoryzator
        with open(vectors_dir / 'vectorizer.pkl', 'rb') as f:
            self.vectorizer = pickle.load(f)

        # Wczytaj metadane
        self.metadata = pd.read_csv(vectors_dir / 'metadata.csv').to_dict('records')

        # Wczytaj dokumenty (opcjonalnie)
        # self.documents = self.load_documents(...)

        print(f"‚úÖ Wektory wczytane z {vectors_dir}")
        return self.vectors

    def search(self, query, top_k=5):
        """
        Szukaj podobnych dokument√≥w za pomocƒÖ cosine similarity.

        Args:
            query: Tekst zapytania lub indeks dokumentu
            top_k: Liczba zwracanych wynik√≥w

        Returns:
            DataFrame z wynikami (nazwa pliku, score, indeks)
        """
        if self.vectors is None:
            raise ValueError("Brak wektor√≥w. Wczytaj lub wektoryzuj dokumenty najpierw.")

        # Je≈õli query to indeks dokumentu
        if isinstance(query, int):
            query_vector = self.vectors[query]
            print(f"üîç Szukam podobnych do dokumentu: {self.metadata[query]['filename']}")
        else:
            # Je≈õli query to tekst
            query_vector = self.vectorizer.transform([query])
            print(f"üîç Szukam podobnych do zapytania: '{query[:50]}...'")

        # Oblicz cosine similarity
        similarities = cosine_similarity(query_vector, self.vectors)[0]

        top_indices = np.argsort(similarities)[::-1]

        results = []
        for idx in top_indices:
            score = similarities[idx]
            if score < 0.1:  # similarity threshold
                continue

            results.append({
                'index': int(idx),
                'filename': self.metadata[idx]['filename'],
                'chunk_id': self.metadata[idx]['chunk_id'],
                'similarity_score': float(score),
                'size': self.metadata[idx]['size']
            })

            if len(results) >= top_k:
                break

        df_results = pd.DataFrame(results)
        return df_results

    def get_feature_names(self):
        """Zwr√≥ƒá nazwy cech (s≈Çowa)."""
        return self.vectorizer.get_feature_names_out()

    def chunk_text(self, text, chunk_size=100, overlap=50):
        """
        Split text into overlapping chunks.
        """
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk = " ".join(words[i:i + chunk_size])
            if len(chunk.strip()) > 0:
                chunks.append(chunk)

        return chunks


# ============================================================================
# G≈Å√ìWNA FUNKCJA - PRZYK≈ÅAD U≈ªYCIA
# ============================================================================

def main():
    """Przyk≈Çad u≈ºycia klasy DocumentVectorizer."""

    # ≈öcie≈ºka do folderu z dokumentami
    documents_folder = r"C:\Users\Albert\Desktop\STUDIA\Sem2\Zastosowanie AI\Regulaminy\txts"
    output_folder = "./vectors_db"

    # Stw√≥rz wektoryzator
    vectorizer = DocumentVectorizer(max_features=5000, ngram_range=(1, 2))

    # Wczytaj dokumenty
    print("=" * 60)
    print("KROK 1: WCZYTYWANIE DOKUMENT√ìW")
    print("=" * 60)
    vectorizer.load_documents(documents_folder)

    # Wektoryzuj dokumenty
    print("\n" + "=" * 60)
    print("KROK 2: WEKTORYZACJA DOKUMENT√ìW")
    print("=" * 60)
    vectorizer.vectorize()

    # Zapisz wektory
    print("\n" + "=" * 60)
    print("KROK 3: ZAPISYWANIE WEKTOR√ìW")
    print("=" * 60)
    vectorizer.save(output_folder)

    # Przyk≈Çad wyszukiwania
    print("\n" + "=" * 60)
    print("KROK 4: PRZYK≈ÅAD WYSZUKIWANIA")
    print("=" * 60)

    # Szukaj podobnych dokument√≥w do pierwszego chunku
    results = vectorizer.search(query=0, top_k=3)
    print("\nüìä Wyniki wyszukiwania (podobne do dokumentu #0):")
    print(results.to_string(index=False))

    # Szukaj na podstawie POLSKIEGO zapytania
    query_text = (
        "Nie wolno przewoziƒá w baga≈ºu pasa≈ºerskim broni palnej i amunicji "
        "z wyjƒÖtkiem broni sportowej i my≈õliwskiej"
    )

    results = vectorizer.search(query=query_text, top_k=7)
    print(f"\nüìä Wyniki wyszukiwania dla zapytania:\n'{query_text}'")
    print(results.to_string(index=False))

    if not results.empty:
        print("\nüîé Dopasowane fragmenty:\n")
        for _, row in results.iterrows():
            idx = row["index"]
            print(f"[{row['filename']} | chunk {row['chunk_id']} | score={row['similarity_score']:.2f}]")
            print(vectorizer.documents[idx][:500])
            print("-" * 80)


if __name__ == "__main__":
    main()
